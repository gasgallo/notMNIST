{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"markdown","source":"This notebook uses different techniques to create models for classification of the images of letters in the dataset.\n\nFirstly we import the libraries we'll need."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"import numpy as np\nimport os\nimport sys\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-white')\nimport seaborn as sns\nfrom PIL import Image\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\n\nimport keras\nfrom keras.preprocessing.image import load_img\nfrom keras.models import Model, Sequential\nfrom keras.layers import Input, Dense, Dropout, Activation, BatchNormalization\nfrom keras.layers import Reshape, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.utils import to_categorical\nfrom keras.utils import np_utils\nfrom keras.regularizers import l1_l2\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n\n%matplotlib inline\n%env JOBLIB_TEMP_FOLDER=/tmp","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"77d2f312278adb0ace9c40a52c16ceadc78f5cc6"},"cell_type":"markdown","source":"Select which data to use."},{"metadata":{"_kg_hide-output":false,"trusted":true,"collapsed":true,"_uuid":"5dc804c8f282dbac69446f4d84b94c989d702c90"},"cell_type":"code","source":"dataset = 'notMNIST_small'\nDATA_PATH = '../input/' + dataset + '/' + dataset","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ef07a200085684ca148f3b3a777a8e3ba3a1e468"},"cell_type":"markdown","source":"Check some data from the training dataset"},{"metadata":{"trusted":true,"_uuid":"f82d2d1e809136bddce06d7bc23fd09e32a110b6","collapsed":true},"cell_type":"code","source":"max_images = 100\ngrid_width = 10\ngrid_height = int(max_images / grid_width)\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\nclasses = os.listdir(DATA_PATH)\nfor j, cls in enumerate(classes):\n    figs = os.listdir(DATA_PATH + '/' + cls)\n    for i, fig in enumerate(figs[:grid_width]):\n        ax = axs[j, i]\n        ax.imshow(np.array(load_img(DATA_PATH + '/' + cls + '/' + fig)))\n        ax.set_yticklabels([])\n        ax.set_xticklabels([])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d663091049d096481d47c09a5493305c8f588edf"},"cell_type":"markdown","source":"Load images and make them ready for fitting a model."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"da1be1bb7c53c1c7c871aa947d7e26fa1c7775d2"},"cell_type":"code","source":"X = []\nlabels = []\n# for each folder (holding a different set of letters)\nfor directory in os.listdir(DATA_PATH):\n    # for each image\n    for image in os.listdir(DATA_PATH + '/' + directory):\n        # open image and load array data\n        try:\n            file_path = DATA_PATH + '/' + directory + '/' + image\n            img = Image.open(file_path)\n            img.load()\n            img_data = np.asarray(img, dtype=np.int16)\n            # add image to dataset\n            X.append(img_data)\n            # add label to labels\n            labels.append(directory)\n        except:\n            None # do nothing if couldn't load file\nN = len(X) # number of images\nimg_size = len(X[0]) # width of image\nX = np.asarray(X).reshape(N, img_size, img_size,1) # add our single channel for processing purposes\nlabels_cat = to_categorical(list(map(lambda x: ord(x)-ord('A'), labels)), 10) # convert to one-hot\nlabels = np.asarray(list(map(lambda x: ord(x)-ord('A'), labels)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7740588dc76628e0d58eb412795604527141e95f"},"cell_type":"markdown","source":"Check balance of classes."},{"metadata":{"trusted":true,"_uuid":"e0b0fb569d96152e7fad6dcfeb864310e461b216","collapsed":true},"cell_type":"code","source":"cls_s = np.sum(labels,axis=0)\n\nfig, ax = plt.subplots()\nplt.bar(np.arange(10), cls_s)\nplt.ylabel('No of pics')\nplt.xticks(np.arange(10), np.sort(classes))\nplt.title('Checking balance for data set..')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0502c16f8a89cc92d00f3e8c4b5454730b925db0"},"cell_type":"markdown","source":"Divide data into train/test datasets."},{"metadata":{"trusted":true,"_uuid":"74e62374fb904c7fda1528329dab6d6f02f4b981","collapsed":true},"cell_type":"code","source":"from sklearn.cross_validation import train_test_split\nX_train,X_valid,y_train,y_valid=train_test_split(X,labels,test_size=0.2)\nX_train_cat,X_valid_cat,y_train_cat,y_valid_cat=train_test_split(X,labels_cat,test_size=0.2)\n\nprint('Training:', X_train.shape, y_train.shape)\nprint('Validation:', X_valid.shape, y_valid.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"50291ff2bfebe0de792e46c2fb0aa780d3e0853c"},"cell_type":"markdown","source":"Let's shuffle the data for a better conditioning of the sets. (useless?)"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c96312c7009b13545b2982e6027d5a60bc7421f2"},"cell_type":"code","source":"# def randomize(dataset, labels):\n#   permutation = np.random.permutation(labels.shape[0])\n#   shuffled_dataset = dataset[permutation,:,:]\n#   shuffled_labels = labels[permutation]\n#   return shuffled_dataset, shuffled_labels\n#   \n# X_train, y_train = randomize(X_train, y_train)\n# X_test, y_test = randomize(X_test, y_test)\n# X_train_cat, y_train_cat = randomize(X_train_cat, y_train_cat)\n# X_test_cat, y_test_cat = randomize(X_test_cat, y_test_cat)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d9446bc6f30435fffb00158fab35f7471ca7fa7"},"cell_type":"markdown","source":"Sanity check of the final dataset."},{"metadata":{"trusted":true,"_uuid":"5f6a1940510822457680781886dfb509a2532c2a","collapsed":true},"cell_type":"code","source":"fig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\nfor j in range(max_images):\n    ax = axs[int(j/grid_width), j%grid_width]\n    ax.imshow(X_train[j,:,:,0])\n    ax.set_yticklabels([])\n    ax.set_xticklabels([])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9792618950397e46e46ca48e525fa2669d8a005d"},"cell_type":"markdown","source":"Prepare the data for the Logistic Regression model."},{"metadata":{"trusted":true,"_uuid":"cfae991abc35ab4b84a5c1123dcbf52e277246ee","collapsed":true},"cell_type":"code","source":"samples, width, height = np.squeeze(X_train).shape\nX_train_lr = np.reshape(X_train,(samples,width*height))\ny_train_lr = y_train\n\nsamples, width, height = np.squeeze(X_valid).shape\nX_valid_lr = np.reshape(X_valid,(samples,width*height))\ny_valid_lr = y_valid","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"76f39f96b92834536486ea26b0e49ac5738ff428"},"cell_type":"markdown","source":"Setup, train and predict with the Logistic Regression model."},{"metadata":{"trusted":true,"_uuid":"56f6fab92341ba7019e798377fdca5336c18817a","collapsed":true},"cell_type":"code","source":"lr = LogisticRegression(multi_class='ovr', solver='saga', random_state=42, \n                        verbose=0, max_iter=5000, n_jobs=-1)\nlr.fit(X_train_lr, y_train_lr)\nlr.score(X_train_lr, y_train_lr)\ny_pred_lr = lr.predict(X_valid_lr)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e37eb44c2b6d582928defad306d44000f3d74e0d"},"cell_type":"markdown","source":"Check accuracy of Logistic Regression model."},{"metadata":{"trusted":true,"_uuid":"5e8da70fbeb16568ecec0a03eab0fb05e60945d5","collapsed":true},"cell_type":"code","source":"from sklearn import metrics\nmetrics.accuracy_score(y_valid_lr, y_pred_lr)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6adaa15d65df93022ded1f7f615c8ecd0c0e3118"},"cell_type":"markdown","source":"Let's test different models.\n\nFirst, a Random Forest one."},{"metadata":{"trusted":true,"_uuid":"e8890c86451cd093e6cd147c09e2c67aff1f01c6","collapsed":true},"cell_type":"code","source":"rf = RandomForestClassifier(n_estimators=1000, criterion=\"gini\", max_depth=10, \n                            min_samples_split=2, min_samples_leaf=1, \n                            n_jobs=-1, random_state=42, verbose=0)\nrf.fit(X_train_lr, y_train_lr)\nrf.score(X_train_lr, y_train_lr)\ny_pred_rf = rf.predict(X_valid_lr)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"974a59bfed8b629472c7d3352f4909491d440b97"},"cell_type":"markdown","source":"Check accuracy of Random Forest model."},{"metadata":{"trusted":true,"_uuid":"46cc95b1248070f7630ca700b2147bac6bf21545","collapsed":true},"cell_type":"code","source":"metrics.accuracy_score(y_valid_lr, y_pred_rf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"821a92473122b71d8aa3c3d98c817d86d2edfd97"},"cell_type":"markdown","source":"What about XGBoost?"},{"metadata":{"trusted":true,"_uuid":"4ef298ea120b88dbd0a82e11e833f5370cc06fa7","collapsed":true},"cell_type":"code","source":"xg_train = xgb.DMatrix(X_train_lr, label=y_train_lr)\nxg_valid = xgb.DMatrix(X_valid_lr, label=y_valid_lr)\n# setup parameters for xgboost\nparam = {}\n# use softmax multi-class classification\nparam['objective'] = 'multi:softmax' # 'multi:softprob'\n# scale weight of positive examples\nparam['eta'] = 0.1\nparam['max_depth'] = 10\nparam['silent'] = 0\nparam['nthread'] = 4\nparam['num_class'] = 10\n\nwatchlist = [(xg_train, 'train'), (xg_valid, 'test')]\nnum_round = 20\nxg = xgb.train(param, xg_train, num_round, \n               watchlist, early_stopping_rounds=50, \n               maximize=True)\n\ny_pred_xg = xg.predict(xg_valid)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"36e4c28a29270267a4596815407d74a3770fe4a3"},"cell_type":"markdown","source":"Check its accuracy."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a0ccca988d210ce82251163bca594aade166cb1c"},"cell_type":"code","source":"metrics.accuracy_score(y_valid_lr, y_pred_xg)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"94edfcc36047fd16ae80afad70168bb5f7a4ad21"},"cell_type":"markdown","source":"Let's start using TensorFlow, specifically I'll use Keras as its wrapper."},{"metadata":{"_uuid":"520c65dd067781f6439d2d192ad58aec13c6210e"},"cell_type":"markdown","source":"The first task is to reproduce the Logistic Regressor.\n\nFirst attempt with \"Adam\" optimizer."},{"metadata":{"trusted":true,"_uuid":"6c995edcaac4e91e40169d85163f1ffe49c5bd48","collapsed":true},"cell_type":"code","source":"def build_logistic_model(resolution, output_dim):\n    model = Sequential()\n    model.add(Flatten(input_shape=(resolution, resolution, 1)))\n    model.add(Dense(output_dim, activation='softmax'))\n    return model\n\ndef plot_training_curves(history):\n    \"\"\"\n    Plot accuracy and loss curves for training and validation sets.\n    Args:\n        history: a Keras History.history dictionary\n    Returns:\n        mpl figure.\n    \"\"\"\n    fig, (ax_acc, ax_loss) = plt.subplots(1, 2, figsize=(8,2))\n    if 'acc' in history:\n        ax_acc.plot(history['acc'], label='acc')\n        if 'val_acc' in history:\n            ax_acc.plot(history['val_acc'], label='Val acc')\n        ax_acc.set_xlabel('epoch')\n        ax_acc.set_ylabel('accuracy')\n        ax_acc.legend(loc='upper left')\n        ax_acc.set_title('Accuracy')\n\n    ax_loss.plot(history['loss'], label='loss')\n    if 'val_loss' in history:\n        ax_loss.plot(history['val_loss'], label='Val loss')\n    ax_loss.set_xlabel('epoch')\n    ax_loss.set_ylabel('loss')\n    ax_loss.legend(loc='upper right')\n    ax_loss.set_title('Loss')\n\n    sns.despine(fig)\n    return\n\nbatch_size = 128\nnb_classes = 10\nnb_epoch = 300\ninput_dim = 784\nresolution = 28\n\nmodel_ada = build_logistic_model(resolution, nb_classes)\n\nmodel_ada.summary()\n\n# compile the model\n# first with gradient descent optimizer\nmodel_ada.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = model_ada.fit(X_train_cat, y_train_cat,\n                    batch_size=batch_size, epochs=nb_epoch,\n                    verbose=0, validation_data=(X_valid_cat, y_valid_cat))\nscore = model_ada.evaluate(X_valid_cat, y_valid_cat, verbose=0)\n\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])\n\nplot_training_curves(history.history)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a5929b7151a8d6c0a6f7773759c5ffd8bc0edd22"},"cell_type":"markdown","source":"With regularization."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"42afdc0897631fa8f960e9fab14cc6cba559e88d"},"cell_type":"code","source":"def build_logistic_model_reg(resolution, output_dim, reg):\n    model = Sequential()\n    model.add(Flatten(input_shape=(resolution, resolution, 1)))\n    model.add(Dense(output_dim, kernel_regularizer=reg, activation='softmax'))\n    return model\n\nreg = l1_l2(l1=0, l2=0.02)\nmodel_ada_reg = build_logistic_model_reg(resolution, nb_classes, reg)\n\nmodel_ada_reg.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = model_ada_reg.fit(X_train_cat, y_train_cat,\n                    batch_size=batch_size, epochs=nb_epoch,\n                    verbose=0, validation_data=(X_valid_cat, y_valid_cat))\nscore = model_ada_reg.evaluate(X_valid_cat, y_valid_cat, verbose=0)\n\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])\n\nplot_training_curves(history.history)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"35389f58873caa6ef3e013463f462d2967e83c23"},"cell_type":"markdown","source":"Let's try also SGD."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"04bd656a82f539cd6eb36da6ed25dcf6ea3f1756"},"cell_type":"code","source":"# then with stochastic gradient descent optimizer\nmodel_sgd = build_logistic_model(resolution, nb_classes)\n\nmodel_sgd.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = model_sgd.fit(X_train_cat, y_train_cat,\n                    batch_size=batch_size, epochs=nb_epoch,\n                    verbose=0, validation_data=(X_valid_cat, y_valid_cat))\nscore = model_sgd.evaluate(X_valid_cat, y_valid_cat, verbose=0)\n\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])\n\nplot_training_curves(history.history)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cb80ff84d2a4a7f23a10339dfa1e948e3fb74744"},"cell_type":"markdown","source":"What if introducing a hidden layer with ReLu activation?"},{"metadata":{"trusted":true,"_uuid":"0db7b1fd640db74334bd5b251b3e130de50f19aa","collapsed":true},"cell_type":"code","source":"def build_logisticHidden_model(resolution, output_dim, hidden_dim):\n    model = Sequential()\n    model.add(Flatten(input_shape=(resolution, resolution, 1)))\n#     kernel_initializer='glorot_normal'\n    model.add(Dense(hidden_dim[0], activation='relu'))\n    model.add(Dense(hidden_dim[1], activation='relu'))\n    model.add(Dense(hidden_dim[2], activation='relu'))\n    model.add(Dense(output_dim, activation='softmax'))\n    return model\n\nhidden_dim = [1024, 300, 50]\n# sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\nmodel_hid = build_logisticHidden_model(resolution, nb_classes, hidden_dim)\n\nmodel_hid.summary()\n\n# compile the model\nmodel_hid.compile(optimizer='adadelta', loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = model_hid.fit(X_train_cat, y_train_cat,\n                    batch_size=batch_size, epochs=nb_epoch,\n                    verbose=0, validation_data=(X_valid_cat, y_valid_cat))\nscore = model_hid.evaluate(X_valid_cat, y_valid_cat, verbose=0)\n\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])\n\nplot_training_curves(history.history)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fa7e5c028922f256873de5e5a842ea86c5ce30fa"},"cell_type":"markdown","source":"With regularization."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"005a8709b54b094a9502eac927ce1d90aec48369"},"cell_type":"code","source":"def build_logisticHidden_model_reg(resolution, output_dim, hidden_dim, reg):\n    model = Sequential()\n    model.add(Flatten(input_shape=(resolution, resolution, 1)))\n    model.add(Dense(hidden_dim[0], kernel_regularizer=reg, activation='relu'))\n    model.add(Dense(hidden_dim[1], kernel_regularizer=reg, activation='relu'))\n    model.add(Dense(hidden_dim[2], kernel_regularizer=reg, activation='relu'))\n    model.add(Dense(output_dim, kernel_regularizer=reg, activation='softmax'))\n    return model\n\nmodel_hid_reg = build_logisticHidden_model_reg(resolution, nb_classes, hidden_dim, reg)\n\nmodel_hid_reg.compile(optimizer='adadelta', loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = model_hid_reg.fit(X_train_cat, y_train_cat,\n                    batch_size=batch_size, epochs=nb_epoch,\n                    verbose=0, validation_data=(X_valid_cat, y_valid_cat))\nscore = model_hid_reg.evaluate(X_valid_cat, y_valid_cat, verbose=0)\n\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])\n\nplot_training_curves(history.history)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"387ad08c8f68edf454a68822955954aa124b3437"},"cell_type":"markdown","source":"With dropout."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"839e90fe808ab9d63c35350498ca58f4fd339626"},"cell_type":"code","source":"def build_logisticHidden_model_dropOut(resolution, output_dim, hidden_dim):\n    model = Sequential()\n    model.add(Flatten(input_shape=(resolution, resolution, 1)))\n    model.add(Dense(hidden_dim[0], activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(hidden_dim[1], activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(hidden_dim[2], activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(output_dim, activation='softmax'))\n    return model\n\nmodel_hid_drop = build_logisticHidden_model_dropOut(resolution, nb_classes, hidden_dim)\n\nmodel_hid_drop.compile(optimizer='adadelta', loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = model_hid_drop.fit(X_train_cat, y_train_cat,\n                    batch_size=batch_size, epochs=nb_epoch,\n                    verbose=0, validation_data=(X_valid_cat, y_valid_cat))\nscore = model_hid_drop.evaluate(X_valid_cat, y_valid_cat, verbose=0)\n\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])\n\nplot_training_curves(history.history)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f4ba3b689e1f34efe0f6234f6b5ac6a3bcf0989"},"cell_type":"markdown","source":"With regularization and dropout."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c099419e13990fd4910b4bb2e65d8875d55d08a5"},"cell_type":"code","source":"def build_logisticHidden_model_regDrop(resolution, output_dim, hidden_dim):\n    model = Sequential()\n    model.add(Flatten(input_shape=(resolution, resolution, 1)))\n    model.add(Dense(hidden_dim[0], kernel_regularizer=reg, activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(hidden_dim[1], kernel_regularizer=reg, activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(hidden_dim[2],kernel_regularizer=reg,  activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(output_dim, kernel_regularizer=reg, activation='softmax'))\n    return model\n\nmodel_hid_regDrop = build_logisticHidden_model_regDrop(resolution, nb_classes, hidden_dim)\n\nmodel_hid_regDrop.compile(optimizer='adadelta', loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = model_hid_regDrop.fit(X_train_cat, y_train_cat,\n                    batch_size=batch_size, epochs=nb_epoch,\n                    verbose=0, validation_data=(X_valid_cat, y_valid_cat))\nscore = model_hid_regDrop.evaluate(X_valid_cat, y_valid_cat, verbose=0)\n\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])\n\nplot_training_curves(history.history)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e782639612de4bd9551add7d0ffb8d7e36c43bec"},"cell_type":"markdown","source":"Let's switch to a more complicated architecture: CNN."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"80676708e1fddb94a0fa957019e2302c2ed1261f"},"cell_type":"code","source":"# input image dimensions\nimg_rows, img_cols = 28, 28\n# number of convolutional filters to use\nnb_filters = 64\n# size of pooling area for max pooling\npool_size = [2, 2]\n# convolution kernel size\nkernel_size = [3, 3]\n\ninput_shape = [img_rows, img_cols, 1]\n\n# model layers\ncnn = Sequential()\ncnn.add(Conv2D(nb_filters, kernel_size, padding='same', input_shape=input_shape))\ncnn.add(Activation('relu'))\ncnn.add(BatchNormalization())\ncnn.add(Conv2D(nb_filters, kernel_size, padding='same'))\ncnn.add(Activation('relu'))\ncnn.add(BatchNormalization())\ncnn.add(MaxPooling2D(pool_size=pool_size))\ncnn.add(Dropout(0.25))\ncnn.add(Flatten())\ncnn.add(Dense(128))\ncnn.add(Activation('relu'))\ncnn.add(BatchNormalization())\ncnn.add(Dropout(0.5))\ncnn.add(Dense(nb_classes))\ncnn.add(Activation('softmax'))\n\ncnn.summary()\n\ncnn.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n\nhistory = cnn.fit(X_train_cat, y_train_cat,\n                 batch_size=batch_size, epochs=nb_epoch,\n                 verbose=0, validation_data=(X_valid_cat, y_valid_cat))\nscore = cnn.evaluate(X_valid_cat, y_valid_cat, verbose=0)\n\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])\n\nplot_training_curves(history.history)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"57035a06cd296b7d1fcee18dde6dc4a3bb212dbe"},"cell_type":"markdown","source":"And then adding some more advance features to make our training process smarter."},{"metadata":{"trusted":true,"_uuid":"7856059cab75d564a22ec9170e5125e853aef424","scrolled":false,"collapsed":true},"cell_type":"code","source":"# define path to save model\nmodel_path = './cnn_notMNIST.h5'\n# prepare callbacks\ncallbacks = [\n    EarlyStopping(\n        monitor='val_acc', \n        patience=20,\n        mode='max',\n        verbose=1),\n    ModelCheckpoint(model_path,\n        monitor='val_acc', \n        save_best_only=True, \n        mode='max',\n        verbose=1),\n    ReduceLROnPlateau(\n        factor=0.1, \n        patience=5, \n        min_lr=0.00001, \n        verbose=1)\n]\n\n# model layers\ncnn_ad = Sequential()\ncnn_ad.add(Conv2D(nb_filters, kernel_size, padding='same', input_shape=input_shape))\ncnn_ad.add(Activation('relu'))\ncnn_ad.add(BatchNormalization())\ncnn_ad.add(Conv2D(nb_filters, kernel_size, padding='same'))\ncnn_ad.add(Activation('relu'))\ncnn_ad.add(BatchNormalization())\ncnn_ad.add(MaxPooling2D(pool_size=pool_size))\ncnn_ad.add(Dropout(0.25))\ncnn_ad.add(Flatten())\ncnn_ad.add(Dense(128))\ncnn_ad.add(Activation('relu'))\ncnn_ad.add(BatchNormalization())\ncnn_ad.add(Dropout(0.5))\ncnn_ad.add(Dense(nb_classes))\ncnn_ad.add(Activation('softmax'))\n\ncnn_ad.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n\nhistory = cnn_ad.fit(X_train_cat, y_train_cat,\n                 batch_size=batch_size, epochs=nb_epoch,\n                 verbose=0, validation_data=(X_valid_cat, y_valid_cat),\n                 shuffle=True, callbacks=callbacks)\nscore = cnn_ad.evaluate(X_valid_cat, y_valid_cat, verbose=0)\n\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])\n\nplot_training_curves(history.history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c15f64c9c38e3c8f63a3ae0c253e05dd2d796e4f"},"cell_type":"markdown","source":"So, with this notebook I've explored different architectures and strategies for classifying the notMNIST dataset, starting from the basics, till state-of-art architectures.\nThe best result is obtained by the CNN (accuracy ~95%) with further possible improvements related to hyper parameters tuning.\nIf you've any comment, question or advice, please do not hesitate to type it down in the comments."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}